{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import polars as pl\n",
    "import implicit\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_meta = pl.read_parquet(\"items_meta.parquet\")\n",
    "users_meta = pl.read_parquet(\"users_meta.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = pl.read_csv('test_pairs.csv')\n",
    "\n",
    "\n",
    "test = test_pairs.with_columns(\n",
    "    pl.col(\"user_id\").cast(pl.UInt32),pl.col(\"item_id\").cast(pl.UInt32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = items_meta.select(\"item_id\",\"source_id\",\"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.3192737178588155e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "train = pl.scan_parquet(\"train_interactions.parquet\")\n",
    "\n",
    "train = train.collect()\n",
    "train = train.join(users_meta,on='user_id').join(it,on = 'item_id')\n",
    "train = train.with_columns(((((pl.col('share')*pl.col('like')).sum()/pl.col(\"share\").sum()).fill_nan(0) - ((pl.col('share')*pl.col('dislike')).sum()/pl.col(\"share\").sum()).fill_nan(0))*pl.col('share')).over(pl.col(\"user_id\")).alias(\"share_user\"))\n",
    "train = train.with_columns(((((pl.col('share')*pl.col('like')).sum()/pl.col(\"share\").sum()).fill_nan(0) - ((pl.col('share')*pl.col('dislike')).sum()/pl.col(\"share\").sum()).fill_nan(0))*pl.col('share')).over(pl.col(\"item_id\")).alias(\"share_item\"))\n",
    "train = train.with_columns(((((pl.col('bookmarks')*pl.col('like')).sum()/pl.col(\"bookmarks\").sum()).fill_nan(0) - ((pl.col('bookmarks')*pl.col('dislike')).sum()/pl.col(\"bookmarks\").sum()).fill_nan(0))*pl.col('bookmarks')).over(pl.col(\"user_id\")).alias(\"bookmarks_user\"))\n",
    "train = train.with_columns(((((pl.col('bookmarks')*pl.col('like')).sum()/pl.col(\"bookmarks\").sum()).fill_nan(0) - ((pl.col('bookmarks')*pl.col('dislike')).sum()/pl.col(\"bookmarks\").sum()).fill_nan(0))*pl.col('bookmarks')).over(pl.col(\"item_id\")).alias(\"bookmarks_item\"))\n",
    "train = train.with_columns((pl.corr(pl.col(\"like\"),pl.col(\"timespent\")/pl.col(\"duration\")).fill_nan(0)*pl.col(\"timespent\")/pl.col(\"duration\")/np.abs(pl.col(\"timespent\")/pl.col(\"duration\")).max() - pl.corr(pl.col(\"dislike\"),pl.col(\"timespent\")/pl.col(\"duration\")).fill_nan(0)*pl.col(\"timespent\")/pl.col(\"duration\")/np.abs(pl.col(\"timespent\")/pl.col(\"duration\")).max()).over(pl.col(\"user_id\")).alias(\"timespent_rel_user\"))\n",
    "# train = train.with_columns((pl.corr(pl.col(\"like\"),pl.col(\"timespent\")/pl.col(\"duration\")).fill_nan(0)*pl.col(\"timespent\")/pl.col(\"duration\")/np.abs(pl.col(\"timespent\")/pl.col(\"duration\")).max() - pl.corr(pl.col(\"dislike\"),pl.col(\"timespent\")/pl.col(\"duration\")).fill_nan(0)*pl.col(\"timespent\")/pl.col(\"duration\")/np.abs(pl.col(\"timespent\")/pl.col(\"duration\")).max()).over(pl.col(\"item_id\")).alias(\"timespent_rel_item\"))\n",
    "# train = train.with_columns((pl.corr(pl.col(\"like\"),pl.col(\"timespent\")).fill_nan(0)*pl.col('timespent')/np.abs(pl.col('timespent')).max() - pl.corr(pl.col(\"dislike\"),pl.col(\"timespent\")).fill_nan(0)*pl.col('timespent')/np.abs(pl.col('timespent')).max()).over(pl.col(\"user_id\")).alias(\"timespent_abs_user\"))\n",
    "train = train.with_columns((pl.corr(pl.col(\"like\"),pl.col(\"timespent\")).fill_nan(0)*pl.col('timespent')/np.abs(pl.col('timespent')).max() - pl.corr(pl.col(\"dislike\"),pl.col(\"timespent\")).fill_nan(0)*pl.col('timespent')/np.abs(pl.col('timespent')).max()).over(pl.col(\"item_id\")).alias(\"timespent_abs_item\"))\n",
    "train = train.with_columns(((pl.col(\"bookmarks_user\")+pl.col(\"bookmarks_item\")+pl.col(\"share_user\")+pl.col(\"share_item\")+pl.col(\"timespent_rel_user\")+pl.col(\"timespent_abs_item\"))/6).alias(\"targets_\"))\n",
    "train = train.drop([\"timespent\",\"timespent_rel_user\",\"bookmarks_user\",\"share_user\",\"timespent_abs_item\",\"bookmarks_item\",\"share_item\"])\n",
    "train = train.with_columns((pl.col(\"targets_\")+pl.col(\"like\")*2-pl.col(\"dislike\")*0.5).alias(\"targets_\"))\n",
    "print(roc_auc_score(train['like'],train['targets_']))\n",
    "print(roc_auc_score(train['dislike'],train['targets_']))\n",
    "train,valid = train_test_split(train,test_size=0.2)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.concat((train,valid.filter(pl.col(\"user_id\").is_in(valid.group_by(\"user_id\").sum().filter((pl.col(\"like\") ==0) | (pl.col(\"dislike\") == 0))['user_id']))))\n",
    "valid = valid.filter(pl.col(\"user_id\").is_in(valid.group_by(\"user_id\").sum().filter((pl.col(\"like\") !=0) & (pl.col(\"dislike\") != 0))['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.filter((pl.col(\"user_id\").is_in(test['user_id'])) | (pl.col(\"item_id\").is_in(test['item_id'])))\n",
    "valid = valid.filter(pl.col(\"user_id\").is_in(test['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = valid.filter(pl.col(\"user_id\").is_in(train.group_by(\"user_id\").sum().filter((pl.col(\"like\") ==0) & (pl.col(\"dislike\") == 0))['user_id']))\n",
    "valid = valid.filter(pl.col(\"user_id\").is_in(train.group_by(\"user_id\").sum().filter((pl.col(\"like\") !=0) | (pl.col(\"dislike\") != 0))['user_id']))\n",
    "train = pl.concat((train,zeros))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertConfig\n",
    "from time import time\n",
    "\n",
    "config = BertConfig(vocab_size=1,hidden_size = 32,num_attention_heads=4,num_hidden_layers=66,intermediate_size=128,max_position_embeddings=6) ##\n",
    "\n",
    "    \n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self,p = 0.1,deviation = 0,min = 0,max = torch.inf,region = 'const'):\n",
    "        super().__init__()\n",
    "        self.mask_dropout = nn.Dropout(p)\n",
    "        self.region = region\n",
    "        assert self.region in ('const','fixed','proc')\n",
    "        self.deviation = deviation\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "    def forward(self,x):\n",
    "        if self.region == 'proc':\n",
    "            region = torch.Tensor(x.detach().cpu()*np.random.choice(np.linspace(-self.deviation,self.deviation,100),x.shape)).to(x.device)\n",
    "        elif self.region == 'const':\n",
    "            region = torch.Tensor(np.random.choice(np.arange(-self.deviation,self.deviation+1),x.shape)).to(x.device)\n",
    "        else:\n",
    "            region = torch.Tensor(np.random.choice(np.arange(self.min,self.max+1),x.shape)).to(x.device)\n",
    "            return ((self.mask_dropout(x.float()).bool().long())*x+(1-self.mask_dropout(x.float()).bool().long())*region).clamp(self.min,self.max)\n",
    "        out = (x+(1-self.mask_dropout(x.float()).bool().long())*region).clamp(self.min,self.max)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class TaskEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.intermidiate = nn.Linear(config.hidden_size,config.intermediate_size)\n",
    "        self.act = nn.GELU()\n",
    "        self.norm = nn.LayerNorm(config.intermediate_size)\n",
    "        self.dropout = nn.Dropout(1)\n",
    "    def forward(self,x):\n",
    "        return self.norm(self.dropout(self.act(self.intermidiate(x).unsqueeze(1)))).squeeze(1)\n",
    "    \n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,config,output_size = 1):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.k = nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.v = nn.Linear(config.hidden_size,config.hidden_size)\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size,output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(1)\n",
    "        self.norm = nn.BatchNorm1d(config.hidden_size)\n",
    "    def forward(self,x1,x2):\n",
    "\n",
    "        x2 = x2.squeeze(2)\n",
    "        x1 = x1.unsqueeze(1)\n",
    "        attention = self.dropout(self.norm((torch.softmax((self.q(x1))@(self.k(x2).permute(0,2,1)),-1)@(self.v(x2))).squeeze(1)))\n",
    "        return self.dense(attention)\n",
    "\n",
    "\n",
    "class Recommender(nn.Module):\n",
    "    def __init__(self,device = None,config = config):\n",
    "        super().__init__()\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.config = config\n",
    "        self.bert = BertModel._from_config(self.config).to(self.device)\n",
    "        self.bert.embeddings.word_embeddings = nn.Dropout(0.1)\n",
    "        self.users_embeddings = nn.Embedding(users_meta.shape[0],config.hidden_size).to(self.device)\n",
    "        self.item_embeddings = nn.Embedding(items_meta.shape[0],config.hidden_size).to(self.device)\n",
    "        self.i_feats = nn.Embedding(items_meta.shape[0],config.hidden_size).to(self.device)\n",
    "        self.i_feats.requires_grad_(False)\n",
    "        self.i_feats.weight.copy_(torch.Tensor(items_meta['embeddings'])).to(self.device)\n",
    "        self.i_feats.requires_grad_(True)\n",
    "        self.i_feats.weight.retain_grad()\n",
    "        \n",
    "\n",
    "        self.item_norm = nn.BatchNorm1d(self.config.hidden_size).to(self.device)\n",
    "        self.source_norm = nn.BatchNorm1d(self.config.hidden_size).to(self.device)\n",
    "\n",
    "\n",
    "        self.source_embeddings = nn.Embedding(19614,config.hidden_size-1).to(self.device)\n",
    "\n",
    "        self.age_embeddings = nn.Embedding(120,config.hidden_size).to(self.device)\n",
    "        self.gender_embeddings = nn.Embedding(3,config.hidden_size).to(self.device)\n",
    "\n",
    "        self.age_dropout = Mask(deviation=2,p=0.2,region='const').to(self.device)\n",
    "        self.gender_dropout = Mask(p = 0.01,min = 0,max = 2,region='fixed').to(self.device)\n",
    "        self.source_dropout = Mask(p = 0.01,min=0,max=0,region='fixed').to(self.device)\n",
    "        self.duration_dropout = Mask(p = 0.4,deviation=0.2,min=0,region='proc').to(self.device)\n",
    "\n",
    "        self.like_head = Classifier(config).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,pairs,users_meta,items_meta):\n",
    "        u = self.users_embeddings(torch.Tensor(pairs['user_id']).to(self.device).long())\n",
    "        u_a = self.age_embeddings(self.age_dropout(torch.Tensor(users_meta[\"age\"]).to(self.device)).long())\n",
    "        u_g = self.gender_embeddings(self.gender_dropout(torch.Tensor(users_meta['gender']).to(self.device)).long())\n",
    "        uu = torch.cat((u.unsqueeze(1),u_a.unsqueeze(1),u_g.unsqueeze(1)),dim=1)\n",
    "\n",
    "        i = self.item_norm(self.item_embeddings(torch.Tensor(pairs['item_id']).to(self.device).long()))\n",
    "        i_f = self.i_feats(torch.Tensor(pairs['item_id']).to(self.device).long())\n",
    "        i_s = self.source_embeddings(self.source_dropout(torch.Tensor(items_meta['source_id']).to(self.device)+1).long()) ## add one to free zeroeth for unknown\n",
    "        i_d = self.duration_dropout(torch.Tensor(items_meta['duration']).to(self.device)).float().unsqueeze(-1)\n",
    "\n",
    "        i_s = self.source_norm(torch.cat((i_s,i_d),dim=-1))\n",
    "        ii = torch.cat((i.unsqueeze(1),i_s.unsqueeze(1),i_f.unsqueeze(1)),dim=1)\n",
    "\n",
    "        output = self.bert(inputs_embeds = torch.cat((uu,ii),dim=1).float(),token_type_ids = torch.cat((torch.zeros(uu.shape[0],uu.shape[1]),torch.ones(ii.shape[0],ii.shape[1])),dim=1).to(self.device).long())['last_hidden_state']\n",
    "        \n",
    "\n",
    "\n",
    "        l_out =  self.like_head(output[:,0,:],output)\n",
    "\n",
    "\n",
    "        return None,l_out,None,None,None\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Recommender('cuda').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ[\"POOL_SIZE\"] = \"24\"\n",
    "BATCH_SIZE = 16000\n",
    "SAMPLE_SIZE = 32\n",
    "class DS_Train(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data.partition_by(\"user_id\", maintain_order=True)\n",
    "        self.data_neg = dict()\n",
    "        self.data_pos = dict()\n",
    "        self.data_neutral = dict()\n",
    "        \n",
    "        for i in tqdm(range(len(self.data)),desc = 'Building DS'):\n",
    "            udata = self.data[i]\n",
    "            self.data_neg[i] = udata.filter(pl.col(\"like\") == 0)\n",
    "            self.data_pos[i] = udata.filter(pl.col(\"like\") == 1)\n",
    "            self.data_neutral[i] = udata.filter(pl.col(\"dislike\") == 0).filter(pl.col(\"like\") == 0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        return 100\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        rand = random.randint(0,len(self.data)-1)\n",
    "        data = self.data[rand]\n",
    "        data = self.data[rand].sample(min(len(data),200))\n",
    "        if len(self.data_pos[index] > 0 ):\n",
    "            pos = self.data_pos[index].sample(min(len(self.data_pos[index]),200))\n",
    "            data = pl.concat((data,pos))\n",
    "        if len(self.data_neg[index] > 0 ):\n",
    "            neg = self.data_neg[index].sample(min(len(self.data_neg[index]),200))\n",
    "            data = pl.concat((data,neg))\n",
    "        if len(self.data_neutral[index] > 0):\n",
    "            neutral = self.data_neutral[index].sample(min(len(self.data_neutral[index]),200))\n",
    "            data = pl.concat((data,neutral))\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "class DS(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data.partition_by(\"user_id\", maintain_order=True)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    sizes = []\n",
    "    for i in data:\n",
    "        sizes.append(i.shape[0])\n",
    "    data = pl.concat(data)\n",
    "    pairs = data.select(\"user_id\",\"item_id\")\n",
    "    users_meta = data.select(\"gender\",\"age\")\n",
    "    items_meta = data.select(\"source_id\",\t\"duration\")\n",
    "    targets = data.select('like','dislike','targets_')\n",
    "    return pairs,users_meta,items_meta,targets,sizes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DS_Train(train)\n",
    "valid_set = DS(valid)\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size=4,shuffle = True,collate_fn=collate_fn,pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set,batch_size = 64, shuffle = False,collate_fn = collate_fn,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped:  bert.embeddings.LayerNorm.weight\n",
      "Skipped:  bert.embeddings.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.0.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.0.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.1.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.1.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.2.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.2.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.3.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.3.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.4.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.4.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.5.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.5.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.6.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.6.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.7.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.7.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.8.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.8.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.9.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.9.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.10.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.10.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.11.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.11.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.12.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.12.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.13.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.13.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.14.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.14.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.15.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.15.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.16.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.16.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.17.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.17.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.18.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.18.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.19.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.19.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.20.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.20.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.21.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.21.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.22.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.22.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.23.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.23.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.24.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.24.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.24.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.24.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.25.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.25.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.25.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.25.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.26.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.26.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.26.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.26.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.27.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.27.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.27.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.27.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.28.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.28.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.28.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.28.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.29.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.29.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.29.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.29.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.30.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.30.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.30.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.30.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.31.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.31.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.31.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.31.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.32.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.32.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.32.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.32.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.33.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.33.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.33.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.33.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.34.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.34.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.34.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.34.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.35.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.35.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.35.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.35.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.36.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.36.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.36.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.36.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.37.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.37.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.37.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.37.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.38.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.38.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.38.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.38.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.39.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.39.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.39.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.39.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.40.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.40.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.40.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.40.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.41.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.41.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.41.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.41.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.42.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.42.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.42.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.42.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.43.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.43.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.43.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.43.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.44.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.44.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.44.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.44.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.45.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.45.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.45.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.45.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.46.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.46.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.46.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.46.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.47.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.47.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.47.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.47.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.48.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.48.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.48.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.48.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.49.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.49.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.49.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.49.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.50.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.50.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.50.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.50.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.51.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.51.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.51.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.51.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.52.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.52.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.52.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.52.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.53.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.53.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.53.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.53.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.54.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.54.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.54.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.54.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.55.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.55.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.55.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.55.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.56.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.56.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.56.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.56.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.57.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.57.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.57.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.57.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.58.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.58.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.58.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.58.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.59.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.59.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.59.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.59.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.60.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.60.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.60.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.60.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.61.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.61.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.61.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.61.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.62.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.62.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.62.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.62.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.63.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.63.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.63.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.63.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.64.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.64.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.64.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.64.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.65.attention.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.65.attention.output.LayerNorm.bias\n",
      "Skipped:  bert.encoder.layer.65.output.LayerNorm.weight\n",
      "Skipped:  bert.encoder.layer.65.output.LayerNorm.bias\n",
      "Skipped:  item_norm.weight\n",
      "Skipped:  item_norm.bias\n",
      "Skipped:  source_norm.weight\n",
      "Skipped:  source_norm.bias\n",
      "Skipped:  like_head.norm.weight\n",
      "Skipped:  like_head.norm.bias\n"
     ]
    }
   ],
   "source": [
    "from bitsandbytes.optim import LAMB\n",
    "def add_weight_decay(model,weight_decay = 1e-2):\n",
    "    decayed = []\n",
    "    no_decayed = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if \"norm\" in name.lower():\n",
    "            print(\"Skipped: \",name)\n",
    "            no_decayed.append(param)\n",
    "        else:\n",
    "            decayed.append(param)\n",
    "    return [{'params': decayed,'weight_decay': weight_decay},\n",
    "            {'params': no_decayed,'weight_decay': 0}]\n",
    "\n",
    "optimizer = LAMB(add_weight_decay(model,1e-3),max_unorm=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "from bidirectional_lambda_loss import LambdaRankLoss\n",
    "lambda_rank_loss_ = LambdaRankLoss(k=1000)\n",
    "\n",
    "def regression_loss(preds, targets):\n",
    "    return torch.mean((preds - targets) ** 2)\n",
    "\n",
    "\n",
    "def margin_based_pairwise_loss(preds, targets, base_margin=0.1, minority_boost=0):\n",
    "    preds_diff = preds.unsqueeze(1) - preds.unsqueeze(0)\n",
    "    targets_diff = targets.unsqueeze(1) - targets.unsqueeze(0)\n",
    "    valid_pairs = (targets_diff > 0).float()\n",
    "\n",
    "    # Dynamic margin: Boost for pairs involving minority class\n",
    "    margins = base_margin + minority_boost * (targets.unsqueeze(1) < -0.5).float()\n",
    "\n",
    "    # Compute loss\n",
    "    loss_matrix = torch.relu(margins - preds_diff) * valid_pairs\n",
    "    num_pairs = valid_pairs.sum()\n",
    "\n",
    "    if num_pairs > 0:\n",
    "        return loss_matrix.sum() / num_pairs\n",
    "    else:\n",
    "        return torch.tensor(0.0, requires_grad=True, device=preds.device)\n",
    "    \n",
    "def lambda_rank_loss(preds,targets):\n",
    "    return lambda_rank_loss_(preds,targets,max((targets==-1).sum(),(targets==1).sum()))\n",
    "    \n",
    "\n",
    "# Total loss\n",
    "def total_loss_fn(preds, targets,classes, lambda0 = 1,lambda1=1e-2):\n",
    "    reg_loss = regression_loss(preds, targets)\n",
    "    \n",
    "    rank_loss = margin_based_pairwise_loss(preds, targets)\n",
    "    lambda_loss = lambda_rank_loss(preds,targets)\n",
    "    return rank_loss*lambda1 + lambda_loss*lambda1 + reg_loss*lambda0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_running_average(total,running_average,step,running_smooth = 0.9999):\n",
    "    if running_average > 0:\n",
    "        running_average_new = (running_average*step + total)/(step+1)\n",
    "        \n",
    "        running_average_new = running_average_new*running_smooth+(1-running_smooth)*total\n",
    "    else:\n",
    "        running_average_new = total\n",
    "    return running_average_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_weights(targets,n_col):\n",
    "    return torch.Tensor([(len(targets)+1)/(targets[:,n_col].sum()+1)]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 9999/45851 [50:30<3:06:22,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.22628476801048963\n",
      "1.166189630744746\n",
      "LR:2.1807594163704172e-05\n",
      "0.7073949013853469\n",
      "0.5043681770174929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2733403427733315\n",
      "0.6759869507524838\n",
      "0.57903790061115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 19999/45851 [1:41:34<2:08:19,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2208686250491999\n",
      "1.1659970766995735\n",
      "LR:4.3617369304922464e-05\n",
      "0.7341578894401819\n",
      "0.5049359816583294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:28<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27506448659631944\n",
      "0.6760817660150447\n",
      "0.5801912239745262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 29999/45851 [2:34:15<1:18:54,  3.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21869342421926558\n",
      "1.1696562034038307\n",
      "LR:6.542714444614077e-05\n",
      "0.7380445847666174\n",
      "0.5046431955641583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27755971815851\n",
      "0.6763852705135156\n",
      "0.5794770939212592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 39999/45851 [3:26:34<30:45,  3.17it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21650826623253525\n",
      "1.1720716001104605\n",
      "LR:8.723691958735906e-05\n",
      "0.7396094575921774\n",
      "0.5057521566293195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27366172704431746\n",
      "0.6765290786581878\n",
      "0.5784909567361117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45851/45851 [3:57:28<00:00,  3.22it/s]   \n",
      "  9%|▉         | 4148/45851 [21:31<3:36:52,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21645037237834186\n",
      "1.1727358280521345\n",
      "LR:9.890953305271423e-05\n",
      "0.7396578179548966\n",
      "0.504890120575076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2776247650384903\n",
      "0.6760186982695243\n",
      "0.5796373460028662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 14148/45851 [1:13:29<2:38:47,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2190732428960502\n",
      "1.1731753245278425\n",
      "LR:9.869143530130205e-05\n",
      "0.7400144808419835\n",
      "0.5052751186189526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2767882800764508\n",
      "0.676049734051376\n",
      "0.5791067703512396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 24148/45851 [2:05:24<1:48:34,  3.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2181830640660599\n",
      "1.1715446575498067\n",
      "LR:9.847333754988987e-05\n",
      "0.7401820905120099\n",
      "0.5063300421218923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2744960175620185\n",
      "0.6764898624149006\n",
      "0.5787292499868995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 34148/45851 [2:57:22<1:00:31,  3.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21592974958308042\n",
      "1.172114017032524\n",
      "LR:9.825523979847768e-05\n",
      "0.7394338717552741\n",
      "0.5050793085875027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2782773266235987\n",
      "0.6761426367132464\n",
      "0.5782504339488684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 44148/45851 [3:49:31<08:44,  3.25it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21766690026167781\n",
      "1.1726514614923171\n",
      "LR:9.80371420470655e-05\n",
      "0.7404679688234806\n",
      "0.5049271805348184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2738300727473365\n",
      "0.6757143835909168\n",
      "0.5785870369497907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45851/45851 [3:58:49<00:00,  3.20it/s]  \n",
      " 18%|█▊        | 8297/45851 [42:46<3:13:57,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2169642016980797\n",
      "1.1721060787388815\n",
      "LR:9.781904429565332e-05\n",
      "0.7401467580533054\n",
      "0.5055655005677714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27414450612333086\n",
      "0.6762411337339626\n",
      "0.5797454348034247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 18297/45851 [1:34:51<2:20:28,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2193870532175526\n",
      "1.171671904365945\n",
      "LR:9.760094654424114e-05\n",
      "0.7403732607420604\n",
      "0.5060242885661513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27297704517841337\n",
      "0.6761316465897472\n",
      "0.5801666123798197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 28297/45851 [2:26:41<1:27:27,  3.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21772771295513957\n",
      "1.1713010604577794\n",
      "LR:9.738284879282895e-05\n",
      "0.7410231819486397\n",
      "0.5044996906452608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2742826090918647\n",
      "0.675752858783553\n",
      "0.5785294958004871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 38297/45851 [3:18:03<37:21,  3.37it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21701690359972417\n",
      "1.172192133170912\n",
      "LR:9.716475104141677e-05\n",
      "0.7410812939514634\n",
      "0.5053446879316817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2733919229772356\n",
      "0.6758426166046984\n",
      "0.5788410046669418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45851/45851 [3:56:55<00:00,  3.23it/s]   \n",
      "  5%|▌         | 2446/45851 [12:22<3:38:50,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21530526905376465\n",
      "1.171781993688569\n",
      "LR:9.694665329000459e-05\n",
      "0.7409876023942458\n",
      "0.5049290048751978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:28<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2747165107064777\n",
      "0.6756587985658887\n",
      "0.5791221724508339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 12446/45851 [1:03:26<2:50:29,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2156733355667442\n",
      "1.1721911271928622\n",
      "LR:9.67285555385924e-05\n",
      "0.7415304575678557\n",
      "0.5047676284307084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27741062508689035\n",
      "0.6761033242127671\n",
      "0.5797719227255332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 22446/45851 [1:54:29<2:03:22,  3.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21790463189594447\n",
      "1.1714221348696705\n",
      "LR:9.651045778718022e-05\n",
      "0.7413466956022923\n",
      "0.5048810330366558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:28<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27728930711746214\n",
      "0.6760320855773507\n",
      "0.5781282470510908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 32446/45851 [2:45:45<1:10:29,  3.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2168555747818202\n",
      "1.1717529637573771\n",
      "LR:9.629236003576804e-05\n",
      "0.7411443129829182\n",
      "0.5052054847240066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2732389367289013\n",
      "0.6761421646208048\n",
      "0.5789690340343857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 42446/45851 [3:37:07<17:58,  3.16it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21885892211385072\n",
      "1.1724330137197294\n",
      "LR:9.607426228435586e-05\n",
      "0.7406839174714555\n",
      "0.505270383716819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27450258566273583\n",
      "0.6758379506365643\n",
      "0.5784290965910245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45851/45851 [3:54:57<00:00,  3.25it/s]  \n",
      " 14%|█▍        | 6595/45851 [33:26<3:24:33,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21842554306220263\n",
      "1.1722087740980736\n",
      "LR:9.585616453294366e-05\n",
      "0.7424445719361379\n",
      "0.5050835076431406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:28<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.2762642674975925\n",
      "0.6759930450269255\n",
      "0.5794235167598581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 16595/45851 [1:24:44<2:30:39,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.21654434976689518\n",
      "1.1717427240207592\n",
      "LR:9.563806678153148e-05\n",
      "0.7411477633607398\n",
      "0.5053473279744377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:29<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID:_____________________________________________________________________________\n",
      "timespent:  0.0\n",
      "roc_star_loss:  0.27492141193813746\n",
      "0.676105552697203\n",
      "0.5779955852648131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 20302/45851 [1:44:23<2:11:22,  3.24it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 225\u001b[0m\n\u001b[0;32m    219\u001b[0m total_l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_l\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    221\u001b[0m running_average_l \u001b[38;5;241m=\u001b[39m compute_running_average(total_l,running_average_l,step)\n\u001b[1;32m--> 225\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# total_loss_ts+= loss_ts.item()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "DIR = \"uw_33333333llll\"\n",
    "writer = SummaryWriter(logdir = DIR)\n",
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "mse = torch.nn.MSELoss()\n",
    "total_loss = 0\n",
    "total_loss_ts = 0\n",
    "total_loss_l = 0\n",
    "total_loss_dl = 0\n",
    "total_loss_s = 0\n",
    "total_loss_likes = 0\n",
    "total_loss_dislikes = 0\n",
    "\n",
    "total_loss_b = 0\n",
    "all_p = []\n",
    "all_pld = []\n",
    "all_t = []\n",
    "all_pd = []\n",
    "all_td = []\n",
    "step = 0\n",
    "model.requires_grad_(True)\n",
    "model.i_feats.requires_grad_(False)\n",
    "\n",
    "\n",
    "eval_size = -1\n",
    "val_steps = 10000\n",
    "\n",
    "NUM_EPOCHS =100\n",
    "WARMUP = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "EVAL_EPOCHS = 1\n",
    "\n",
    "\n",
    "\n",
    "total_l = 0\n",
    "total_dl = 0\n",
    "running_average_l = 0\n",
    "running_average_dl = 0\n",
    "running_smooth = 0.9999\n",
    "\n",
    "\n",
    "auc_l= 0\n",
    "auc_dl = 0\n",
    "auc_pld = 0\n",
    "auc_pdl = 0\n",
    "\n",
    "\n",
    "all_p = []\n",
    "all_t = []\n",
    "all_p_dl = []\n",
    "all_t_dl = []\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "\n",
    "    for pairs,users_meta_,items_meta_,targets,sizes in tqdm(train_loader):\n",
    "        if step % 100 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        miss_l = 0\n",
    "        miss_dl = 0\n",
    "        _auc_l= 0\n",
    "        _auc_dl = 0\n",
    "\n",
    "        likes = torch.Tensor(targets['like']).cuda()\n",
    "        dislikes = torch.Tensor(targets['dislike']).cuda()\n",
    "\n",
    "        general_targets = torch.cat((dislikes.unsqueeze(1),likes.unsqueeze(1)),dim=1).cuda()\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        # freeze_bn(model)\n",
    "        drops = np.random.randn(1000)\n",
    "        drops -= drops.min()\n",
    "        drops/= drops.max()\n",
    "        for name,module in model.named_modules():\n",
    "            if isinstance(module,nn.Dropout) and 'mask' not in name:\n",
    "                if 'embedding' not in name:\n",
    "                    module.p = np.clip(np.random.choice(drops)**2,0.4,0.9)\n",
    "                else:\n",
    "                    module.p = np.clip(np.random.choice(drops)**2,0.1,0.2)\n",
    "\n",
    "\n",
    "\n",
    "        if e < WARMUP:\n",
    "            for i in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[i]['lr'] = LEARNING_RATE * step/(len(train_loader)*WARMUP)\n",
    "        else:\n",
    "            for i in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[i]['lr'] = LEARNING_RATE - LEARNING_RATE*step/(len(train_loader))/NUM_EPOCHS\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ts,out,dl,share,bookmarks = model(pairs,users_meta_,items_meta_)\n",
    "        \n",
    "\n",
    "        loss = 0\n",
    "        loss_l = 0\n",
    "        loss_s = 0\n",
    "        loss_b = 0\n",
    "        loss_ts = 0\n",
    "        loss_like = 0\n",
    "        loss_dislike = 0\n",
    "        u = 0\n",
    "        for u_n in sizes:\n",
    "            u_out = out[u:u+u_n]\n",
    "            u_targets = targets[u:u+u_n]\n",
    "            l = u_out\n",
    "            u_general_targets = general_targets[u:u+u_n]\n",
    "\n",
    "\n",
    "            targets_ = torch.Tensor(u_targets['targets_']).cuda()\n",
    "            loss_l_ = torch.mean(torch.binary_cross_entropy_with_logits(l.squeeze(1),torch.Tensor(u_targets['like']).cuda(),pos_weight = log_weights(u_targets.to_numpy(),0)))\n",
    "            loss_ts_ = total_loss_fn(l.squeeze(1),targets_,torch.Tensor(u_targets['like']).cuda() - torch.Tensor(u_targets['dislike']).cuda())\n",
    "\n",
    "\n",
    "            loss+= loss_ts_\n",
    "\n",
    "\n",
    "\n",
    "            if u_general_targets[:,1].sum() == 0 or u_general_targets[:,1].sum() == len(u_general_targets[:,1]):\n",
    "\n",
    "                _auc_l += 0.5\n",
    "            else:\n",
    "                _auc_l += roc_auc_score(u_general_targets[:,1].detach().cpu(),u_out.detach().cpu())\n",
    "\n",
    "            if u_general_targets[:,0].sum() == 0 or u_general_targets[:,0].sum() == len(u_general_targets[:,0]):\n",
    "\n",
    "                _auc_dl += 0.5\n",
    "            else:\n",
    "                _auc_dl += roc_auc_score(u_general_targets[:,0].detach().cpu(),u_out.detach().cpu()*-1)\n",
    "            u+=u_n\n",
    "\n",
    "            loss_l += loss_l_\n",
    "            loss_ts += loss_ts_\n",
    "                \n",
    "        try:        \n",
    "            auc_l += _auc_l/(len(sizes))\n",
    "        except ZeroDivisionError:\n",
    "            auc_l+=0.5/len(sizes)\n",
    "\n",
    "        try:\n",
    "            auc_dl += _auc_dl/(len(sizes))  \n",
    "        except ZeroDivisionError:\n",
    "            auc_dl+=0.5/len(sizes)\n",
    "        \n",
    "\n",
    "        loss_l = loss_l/len(sizes)\n",
    "        loss_ts = loss_ts/len(sizes)\n",
    "        \n",
    "        \n",
    "        total_loss += loss_ts.item()\n",
    "\n",
    "        total_l += loss_l.item()\n",
    "\n",
    "        running_average_l = compute_running_average(total_l,running_average_l,step)\n",
    "\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if step % val_steps == val_steps-1:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"TRAIN:_____________________________________________________________________________\")\n",
    "            print(\"timespent: \", total_loss_ts/val_steps)\n",
    "            print(\"roc_star_loss: \", total_loss/val_steps)\n",
    "            print(running_average_l)\n",
    "            print(f\"LR:{optimizer.param_groups[0]['lr']}\")\n",
    "            writer.add_scalar(\"train_roc_star\",total_loss/val_steps,step)\n",
    "            writer.add_scalar(\"train_running\",running_average_l,step)\n",
    "            writer.add_scalar(\"learning_rate\",optimizer.param_groups[0]['lr'],step)\n",
    "            writer.add_scalar(\"train_auc_l\",auc_l/(val_steps-miss_l),step)\n",
    "            writer.add_scalar(\"train_auc_dl\",auc_dl/(val_steps-miss_dl),step)\n",
    "\n",
    "            print(auc_l/(val_steps))     \n",
    "            print(auc_dl/(val_steps))\n",
    "            auc_l= 0\n",
    "            auc_dl = 0\n",
    "            auc_pld = 0\n",
    "            auc_pdl = 0\n",
    "            miss_l = 0\n",
    "            miss_dl = 0\n",
    "\n",
    "            total_loss_ts = 0\n",
    "            total_loss_l = 0\n",
    "            total_loss_dl = 0\n",
    "            total_l_loss = 0\n",
    "            total_loss_s = 0\n",
    "            total_loss_b = 0\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "            torch.save(model.state_dict(),f\"{DIR}/cp.pt\")\n",
    "            torch.save(optimizer.state_dict(),f\"{DIR}/optim.pt\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "            import seaborn as sns\n",
    "            import matplotlib.pyplot as plt\n",
    "            num_heads = model.config.num_attention_heads\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            vtotal_loss_ts = 0\n",
    "            vtotal_loss_l = 0\n",
    "            vtotal_loss_dl = 0\n",
    "\n",
    "            vtotal_loss = 0\n",
    "            \n",
    "            vtotal_l_loss = 0\n",
    "            model.eval()\n",
    "            vauc_l = 0\n",
    "            vauc_dl = 0\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            with torch.no_grad():\n",
    "                        for i,(pairs,users_meta_,items_meta_,targets,sizes) in enumerate(tqdm(valid_loader)):\n",
    "                            vmiss_l = 0\n",
    "                            vmiss_dl = 0\n",
    "                    \n",
    "                            \n",
    "                            ts,out,dl,share,bookmarks = model(pairs,users_meta_,items_meta_)\n",
    "                            \n",
    "                            vloss = 0\n",
    "                            vp_loss = 0\n",
    "                            vl_loss = 0\n",
    "                            vloss_s = 0\n",
    "                            vloss_b = 0\n",
    "                            vloss_like = 0\n",
    "                            vloss_ts = 0\n",
    "                            vloss_dislike = 0\n",
    "                            u = 0\n",
    "                            _vauc_l = 0\n",
    "                            _vauc_dl = 0\n",
    "\n",
    "                            for u_n in sizes:\n",
    "                                u_out = out[u:u+u_n]\n",
    "                                u_targets = targets[u:u+u_n]\n",
    "                                l = u_out\n",
    "\n",
    "                                targets_ = torch.Tensor(u_targets['targets_']).cuda()\n",
    "\n",
    "                                vl_loss += total_loss_fn(l.squeeze(1),targets_,torch.Tensor(u_targets['like']).cuda() - torch.Tensor(u_targets['dislike']).cuda())\n",
    "\n",
    "                                if u_targets[:,0].sum() == 0 or u_targets[:,0].sum() == len(u_targets[:,0]):\n",
    "                                    vmiss_l +=1\n",
    "                                else:\n",
    "                                    _vauc_l += roc_auc_score(u_targets[:,0],u_out.detach().cpu())\n",
    "\n",
    "                                if u_targets[:,1].sum() == 0 or u_targets[:,1].sum() == len(u_targets[:,1]):\n",
    "                                    vmiss_dl +=1\n",
    "                                else:\n",
    "                                    _vauc_dl += roc_auc_score(u_targets[:,1],u_out.detach().cpu()*-1)\n",
    "                                u+=u_n\n",
    "\n",
    "                            vl_loss/= len(sizes)\n",
    "\n",
    "\n",
    "                            try:\n",
    "                                vauc_l += _vauc_l/(len(sizes)-vmiss_l)\n",
    "                            except ZeroDivisionError:\n",
    "                                vauc_l += 0.5/len(sizes)\n",
    "                            \n",
    "                            try:\n",
    "                                vauc_dl += _vauc_dl/(len(sizes)-vmiss_dl)  \n",
    "                            except ZeroDivisionError:\n",
    "                                vauc_dl += 0.5 / len(sizes)\n",
    "                            vloss= vl_loss\n",
    "                            \n",
    "                            \n",
    "                                    \n",
    "                            vtotal_loss += vloss.item()\n",
    "\n",
    "\n",
    "                            if i == eval_size:\n",
    "                                break\n",
    "\n",
    "            print(\"VALID:_____________________________________________________________________________\")\n",
    "            print(\"timespent: \", vtotal_loss_ts/(i+1))\n",
    "            print(\"roc_star_loss: \", vtotal_loss/(i+1))\n",
    "\n",
    "            print(vauc_l/(i+1))\n",
    "            print(vauc_dl/(i+1))\n",
    "            writer.add_scalar(\"valid_timespent\",vtotal_loss_ts/(i+1),step)\n",
    "            writer.add_scalar(\"valid_roc_star\",vtotal_loss/(i+1),step)\n",
    "\n",
    "            writer.add_scalar(\"roc_auc\",vauc_l/(i+1),step)\n",
    "            writer.add_scalar(\"roc_auc_dl\",vauc_dl/(i+1),step)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            torch.save(model.state_dict(),f\"{DIR}/{e}_{vauc_l/(i+1)}.pt\")\n",
    "            torch.save(optimizer.state_dict(),f\"{DIR}/{e}_{vauc_l/(i+1)}optim.pt\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_l = 0 \n",
    "        step+=1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = pl.read_csv('test_pairs.csv')\n",
    "\n",
    "\n",
    "test = test_pairs.with_columns(\n",
    "    pl.col(\"user_id\").cast(pl.UInt32),pl.col(\"item_id\").cast(pl.UInt32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS_Test(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_test(data):\n",
    "    data = pl.concat(data)\n",
    "    pairs = data.select(\"user_id\",\"item_id\")\n",
    "    users_meta = data.select(\"gender\",\"age\")\n",
    "    items_meta = data.select(\"source_id\",\t\"duration\")\n",
    "    return pairs,users_meta,items_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.join(users_meta,on='user_id').join(it,on = 'item_id')\n",
    "test_ds = DS_Test(test_pairs)\n",
    "test_loader = DataLoader(test,shuffle=False,batch_size=1024,collate_fn=collate_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1617/1617 [00:55<00:00, 29.12it/s]\n"
     ]
    }
   ],
   "source": [
    "predict = None\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for pairs,users_meta_,items_meta_ in tqdm(test_loader):\n",
    "        if predict is None:\n",
    "            predict = model(pairs,users_meta_,items_meta_)[1].squeeze().detach().cpu().numpy()\n",
    "        else:\n",
    "            predict = np.concatenate((predict,model(pairs,users_meta_,items_meta_)[1].squeeze().detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14886862"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs.with_columns(predict=predict).write_csv('sample_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
