{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 131,716,096 || all params: 434,025,472 || trainable%: 30.3475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dls-workshop\\utils.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  p = torch.load(f\"{cp}/{name}\").requires_grad_(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 427,501,568 || trainable%: 0.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dls-workshop\\utils.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  p = torch.load(f\"{cp}/{name}\").requires_grad_(False)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from model_coupled import CoupledBert\n",
    "from classifier import Classifier\n",
    "from nli import NLIModel\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "from utils import load_model\n",
    "import numpy as np\n",
    "\n",
    "cbert_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "cbert_tokenizer.add_tokens(['[QUERY]','[DESCRIPTION]','[NUM]'])\n",
    "cbert = BertModel.from_pretrained(\"sbert107\")\n",
    "cbert.resize_token_embeddings(cbert.embeddings.word_embeddings.weight.shape[0]+3)\n",
    "\n",
    "\n",
    "cmodel = Classifier(cbert,cbert_tokenizer,mask_prob=0)\n",
    "cmodel.eval()\n",
    "\n",
    "load_model(cmodel,\"gg_/checkpoint-1200\")\n",
    "cmodel.requires_grad_(False)\n",
    "\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "bert_tokenizer.add_tokens(['[QUERY]','[DESCRIPTION]','[NUM]'])\n",
    "bert = BertModel.from_pretrained(\"10-8nli\")\n",
    "bert.eval()\n",
    "top_k = 50\n",
    "\n",
    "model = NLIModel(bert,bert_tokenizer,embedding_dim=512,k=top_k,mask_prob=0)\n",
    "load_model(model,\"10-8mean_sbert8___/checkpoint-404\")\n",
    "bert.requires_grad_(False)\n",
    "\n",
    "\n",
    "\n",
    "model = CoupledBert(model,bert_tokenizer,top_k,50,cmodel,cbert_tokenizer).cuda()\n",
    "model.eval()\n",
    "model.prepare_description_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Map: 100%|██████████| 22188/22188 [00:00<00:00, 31489.93 examples/s]\n",
      "100%|██████████| 15/15 [00:07<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from data import train_dataset,test_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def collate(x):\n",
    "    out = {\"text\":[],\"labels\":[]}\n",
    "    for i in x:\n",
    "        out['text'].append(i['text'])\n",
    "        out['labels'].append(i['labels'])\n",
    "    return out\n",
    "\n",
    "test_loader = DataLoader(test_dataset,batch_size=64,collate_fn=collate)\n",
    "\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_sims = []\n",
    "test_classes = []\n",
    "test_descs = []\n",
    "tp = []\n",
    "test_embeddings = []\n",
    "model.eval()\n",
    "for xx in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        pred = model.predict(xx['text'],k=top_k,labels=torch.Tensor(xx['labels']))\n",
    "        cls_token,desc_token,classes,preds,labels = pred\n",
    "        desc_token = desc_token.view(cls_token.shape[0],-1,512)\n",
    "        test_classes.extend(classes.detach().cpu().numpy().tolist())\n",
    "        for i in range(cls_token.shape[0]):\n",
    "            test_sims.extend(torch.nn.functional.cosine_similarity(cls_token[i],desc_token[i]).detach().cpu().numpy().tolist())\n",
    "        test_descs.extend(desc_token.detach().cpu().numpy().tolist())\n",
    "        test_preds.extend(cls_token.detach().cpu().numpy().tolist())\n",
    "        tp.extend(preds.detach().cpu().numpy().tolist())\n",
    "        test_labels.extend(xx['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descs = np.array(test_descs).squeeze()\n",
    "test_classes = np.array(test_classes).squeeze()\n",
    "test_labels = np.array(test_labels).astype(int).squeeze()\n",
    "test_preds = np.array(test_preds).squeeze()\n",
    "test_y = []\n",
    "test_classes = test_classes.reshape(test_preds.shape[0],top_k)\n",
    "test_x = np.stack((np.repeat(np.expand_dims(test_preds,1),top_k,1),test_descs.reshape(test_preds.shape[0],top_k,-1)),axis=-1).reshape(-1,top_k,1024)\n",
    "for i in range(test_classes.shape[0]):\n",
    "    test_y.append(test_labels[i][test_classes.astype(int)[i]])\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.array(tp)\n",
    "test_labels = np.array(test_labels)\n",
    "test_classes = np.array(test_classes).reshape(test_labels.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = torch.sigmoid(torch.Tensor(tp)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 509.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5145945945945946 F1:  0.6795146324054246 Accuracy-flat:  0.5145945945945946 top_k:  1 threshold:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 101/101 [00:00<00:00, 508.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5286486486486487 F1:  0.6714997734481196 Accuracy-flat:  0.6081081081081081 top_k:  2 threshold:  0.76\n",
      "Accuracy:  0.547027027027027 F1:  0.674301420064132 Accuracy-flat:  0.6156756756756757 top_k:  2 threshold:  0.77\n",
      "Accuracy:  0.5675675675675675 F1:  0.6777932313398238 Accuracy-flat:  0.6243243243243243 top_k:  2 threshold:  0.78\n",
      "Accuracy:  0.572972972972973 F1:  0.6747777257838091 Accuracy-flat:  0.6243243243243243 top_k:  2 threshold:  0.79\n",
      "Accuracy:  0.5772972972972973 F1:  0.6701372456223379 Accuracy-flat:  0.6232432432432432 top_k:  2 threshold:  0.8\n",
      "Accuracy:  0.5827027027027027 F1:  0.6660287081339713 Accuracy-flat:  0.6227027027027027 top_k:  2 threshold:  0.81\n",
      "Accuracy:  0.5881081081081081 F1:  0.6653790439401256 Accuracy-flat:  0.6254054054054055 top_k:  2 threshold:  0.8200000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 891.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5945945945945946 F1:  0.6710044733631557 Accuracy-flat:  0.7084684684684684 top_k:  3 threshold:  0.8\n",
      "Accuracy:  0.5978378378378378 F1:  0.663358147229115 Accuracy-flat:  0.7066666666666667 top_k:  3 threshold:  0.81\n",
      "Accuracy:  0.6064864864864865 F1:  0.663029794376836 Accuracy-flat:  0.7106306306306306 top_k:  3 threshold:  0.8200000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 834.70it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 756.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6075675675675676 F1:  0.6451116243264049 Accuracy-flat:  0.8006486486486486 top_k:  5 threshold:  0.8200000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 663.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6097297297297297 F1:  0.6456219466366028 Accuracy-flat:  0.83009009009009 top_k:  6 threshold:  0.8200000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████▏   | 63/101 [00:00<00:00, 629.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6108108108108108 F1:  0.6444444444444445 Accuracy-flat:  0.8517374517374517 top_k:  7 threshold:  0.8200000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 603.81it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 576.80it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 538.29it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 495.95it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 333.75it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 447.39it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 415.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6118918918918919 F1:  0.6451378809869376 Accuracy-flat:  0.9186694386694386 top_k:  13 threshold:  0.8200000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 412.07it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 388.63it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 371.52it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 356.65it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 340.92it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 329.55it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 242.75it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 306.98it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 292.31it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 280.41it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 273.90it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 264.51it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 256.99it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 251.56it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 199.75it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 236.39it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 230.22it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 222.95it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 217.64it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 215.52it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 208.21it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 200.95it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 196.09it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 165.60it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 191.12it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 185.91it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 182.76it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 179.34it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 176.43it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 168.42it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 168.61it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 162.86it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 140.72it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 160.58it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 155.50it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 154.19it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 151.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,classification_report\n",
    "max_acc = 0\n",
    "max_acc_f = 0\n",
    "max_f1 = 0\n",
    "max_t = np.array([0,0,0,0])\n",
    "max_k = 1\n",
    "max_tc = 0\n",
    "r = dict()\n",
    "ds = np.array(train_dataset['labels'])\n",
    "\n",
    "weights = np.array([i/len(ds) for i in ds.sum(axis = 0)])\n",
    "weights -= weights.min()\n",
    "weights /= weights.max()\n",
    "\n",
    "\n",
    "for k in range(1,51):\n",
    "    t_labels = []\n",
    "    t_classes = []\n",
    "    miss = 0\n",
    "    for i in range(test_labels.shape[0]):\n",
    "        t_classes.append(tp[i].argsort()[-k:])\n",
    "        t_miss = test_labels[i][tp[i].argsort()[:-k]].sum()>0\n",
    "        t_labels.append(test_labels[i][t_classes[-1]].tolist())\n",
    "        if t_miss:\n",
    "            miss+=1\n",
    "            t_labels[-1]= [i*0 for i in t_labels[-1]]\n",
    "    t_labels = np.array(t_labels)\n",
    "\n",
    "    test_sims = np.array(test_sims).reshape(test_labels.shape)\n",
    "    t_sims= []\n",
    "    for i in range(test_labels.shape[0]):\n",
    "        p = test_sims[i][np.argsort(test_classes[i])][tp[i].argsort()[-k:]]\n",
    "        t_miss = test_labels[i][tp[i].argsort()[:-k]].sum()>0\n",
    "        p[p.argmax()] = 1\n",
    "        t_sims.append(p)\n",
    "        if t_miss:\n",
    "            t_sims[-1] = [1 for i in t_sims[-1]]\n",
    "    t_sims = np.array(t_sims)\n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    \n",
    "\n",
    "    for thresh in tqdm(np.linspace(0,1,101)):\n",
    "                \n",
    "                acc = accuracy_score(t_labels.tolist(),(t_sims>thresh).astype(int).tolist())\n",
    "                if  acc > max_acc:\n",
    "                    max_acc = acc\n",
    "                    max_acc_f = accuracy_score(t_labels.ravel().tolist(),((t_sims>thresh)).astype(int).ravel().tolist())\n",
    "                    max_f1 = f1_score(t_labels.ravel().tolist(),((t_sims>thresh)).astype(int).ravel().tolist())\n",
    "                    max_t = thresh.copy()\n",
    "                    max_k = k\n",
    "                    max_t = thresh\n",
    "\n",
    "                    print(\"Accuracy: \",acc,\"F1: \", max_f1,\"Accuracy-flat: \" ,max_acc_f, \"top_k: \", max_k, f\"threshold: \", thresh)\n",
    "                    \n",
    "                elif acc == max_acc:\n",
    "                    f1 = f1_score(t_labels.ravel().tolist(),((t_sims*weights[t_classes[i]]>thresh)).astype(int).ravel().tolist())\n",
    "                    if f1 > max_f1:\n",
    "                        max_f1 = f1\n",
    "                        max_acc_f = f1_score(t_labels.ravel().tolist(),((t_sims>thresh)).astype(int).ravel().tolist())\n",
    "                        print(\"Accuracy: \",acc,\"F1: \", max_f1,\"Accuracy-flat: \" ,max_acc_f, \"top_k: \", max_k, f\"threshold: \", thresh)\n",
    "                        max_t = thresh.copy()\n",
    "                        max_k = k\n",
    "                        max_t = thresh\n",
    "\n",
    "\n",
    "                    elif max_f1 == f1:\n",
    "                        acc_f = accuracy_score(t_labels.ravel().tolist(),((t_sims>thresh)).astype(int).ravel().tolist())\n",
    "                        if acc_f > max_acc_f:\n",
    "                            max_acc_f = acc_f\n",
    "                            print(\"Accuracy: \",acc,\"F1: \", max_f1,\"Accuracy-flat: \" ,max_acc_f, \"top_k: \", max_k, f\"threshold: \", thresh)\n",
    "                            max_t = thresh.copy()\n",
    "                            max_k = k\n",
    "                            max_t = thresh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
