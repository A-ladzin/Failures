{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 131,716,096 || all params: 434,025,472 || trainable%: 30.3475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dls-workshop\\utils.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  p = torch.load(f\"{cp}/{name}\").requires_grad_(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 427,501,568 || trainable%: 0.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dls-workshop\\utils.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  p = torch.load(f\"{cp}/{name}\").requires_grad_(False)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from model_coupled import CoupledBert\n",
    "from classifier import Classifier\n",
    "from nli import NLIModel\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "from utils import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "cbert_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "cbert_tokenizer.add_tokens(['[QUERY]','[DESCRIPTION]','[NUM]'])\n",
    "cbert = BertModel.from_pretrained(\"sbert107\")\n",
    "cbert.resize_token_embeddings(cbert.embeddings.word_embeddings.weight.shape[0]+3)\n",
    "#В данном случае классификатор лишь забирает ресурсы\n",
    "# Его смысл в том, сужать диапазон классов, и оставлять только top-k для последующей зеро-шот классификации, \n",
    "# регулируя в параметр k можно избавить модель от некоторого количества ошибок первого рода, что может увеличить качество при правильном обучении\n",
    "# Но в последнем чек-поинте модель лучше справляется с максимальным топ-k = N классов, тем не менее является частью решения, и возможно выигрышным был сабмит с топ_к = 9 и с весами, которые не сохранились\n",
    "cmodel = Classifier(cbert,cbert_tokenizer,mask_prob=0)\n",
    "cmodel.eval()\n",
    "\n",
    "load_model(cmodel,\"gg_/checkpoint-1200\")\n",
    "cmodel.requires_grad_(False)\n",
    "\n",
    "\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "bert_tokenizer.add_tokens(['[QUERY]','[DESCRIPTION]','[NUM]'])\n",
    "bert = BertModel.from_pretrained(\"10-8nli\")\n",
    "bert.eval()\n",
    "top_k = 50\n",
    "model = NLIModel(bert,bert_tokenizer,embedding_dim=512,k=top_k,mask_prob=0)\n",
    "# Не могу быть уверенным, что именно этот чекпоинт дал результат в приватном либердорте, но это последний топ-1 на паблике, если выиграл тот, который был 2 дня назад, то, он должен быть в одной из тех папочек, только возможно придется поменять количество и размер адаптеров, чтобы загружались без предупреждений, бест трешолды и топ-к можно вычислить в ноутбуке evaluation, там всё для этого захардкодено\n",
    "load_model(model,\"10-8mean_sbert8___/checkpoint-404\")\n",
    "bert.requires_grad_(False)\n",
    "\n",
    "\n",
    "\n",
    "model = CoupledBert(model,bert_tokenizer,top_k,50,cmodel,cbert_tokenizer).cuda()\n",
    "model.eval()\n",
    "\n",
    "model.prepare_description_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Map: 100%|██████████| 22188/22188 [00:00<00:00, 30810.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from data import train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = pathlib.Path().absolute()\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "RANDOM_SEED = 42\n",
    "test = pd.read_csv(DATA_DIR / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9015/9015 [05:22<00:00, 27.98it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(test['text']):\n",
    "        \n",
    "        if isinstance(i, str):\n",
    "            i = '[QUERY]'+i\n",
    "            outputs,desc,classes,_ = model.predict(i,k=top_k)\n",
    "        else:\n",
    "            outputs,desc,classes,_ = model.predict('[QUERY].',k=top_k)\n",
    "        \n",
    "        desc = desc.squeeze(1)\n",
    "\n",
    "        similarities = torch.nn.functional.cosine_similarity(outputs,desc)\n",
    "        preds = classes[torch.where(similarities > 0.819)].sort().values.detach().cpu().numpy().astype(int).astype(str).tolist()\n",
    "\n",
    "        if len(preds)==0:\n",
    "            preds = [classes[torch.argmax(similarities)].sort().values.detach().cpu().numpy().astype(int).astype(str).tolist()]\n",
    "            \n",
    "        result.append(' '.join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(np.hstack([test[\"index\"].values.reshape(-1,1), np.array(result).reshape(-1,1)]),\n",
    "                  columns = [\"index\",\"target\"])\n",
    "res.to_csv(DATA_DIR / \"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
